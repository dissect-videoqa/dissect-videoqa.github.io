<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality Fusion">
  <meta name="keywords" content="VideoQA, Interpretable AI, XAI, Explainable AI, Multimodal, Video-Text">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality Fusion</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-5JX0F75QDW"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://dissect-videoqa.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          Related Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://gradslam.github.io">
            GradSLAM
          </a>
          <a class="navbar-item" href="https://mahis.life/clip-fields/">
            CLIP-Fields
          </a>
          <a class="navbar-item" href="https://pengsongyou.github.io/openscene">
            OpenScene
          </a>
          <a class="navbar-item" href="https://say-can.github.io/">
            Say-Can
          </a>
        </div>
      </div>
    
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality Fusion</h1>
          <h2 class="title is-6 publlication-title">International Conference on Machine Learning (ICML) 2024</h2>
          <div class="is-size-6 publication-authors">
            <span class="author-block">
              <a href="https://israwal.github.io">Ishaan Singh Rawal</a> 
              <a href="mailto:ishaanrawal@gmail.com" target="_blank"><button style="font-size:14px"><i class="fa fa-envelope"></i></button></a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=9GDRHp8AAAAJ&hl=en">Alexander Matyasko</a>,</span>
            <span class="author-block">
              <a href="https://shantanuj.github.io/">Shantanu Jaiswal</a>,
            </span>
            <span class="author-block">
              <a href="https://basurafernando.github.io/">Basura Fernando</a>,
            </span>
            <span class="author-block">
              <a href="https://www.a-star.edu.sg/cfar/about-cfar/our-team/dr-cheston-tan">Cheston Tan</a>
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block">Agency for Science, Technology and Research (A*STAR), Singapore</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://icml.cc/virtual/2024/poster/33856"
                   class="external-link button is-normal ">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2306.08889"
                   class="external-link button is-normal ">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=rkXgws8fiDs"
                   class="external-link button is-normal ">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/israwal/dissect-videoqa"
                   class="external-link button is-normal ">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="#"
                   class="external-link button is-normal" disabled="true">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span> -->
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <video poster="" id="clustering" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/clustering.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="roundabout-nav" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/roundabout nav.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="tm-baymax" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tm baymax.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="text-query-somewhere-to-sit" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/text query somewhere to sit.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="image-query-cabinet" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/image query cabinet.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="audio-query-doorknock" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/audio query doorknock.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="click-query-cabinet" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/click query cabinet.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="clustering-outdoors" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/clustering outdoors.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="football-field-nav" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/football field nav.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="tm-caterpillar" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tm caterpillar.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="tm-purple" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tm purple.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item">
          <video poster="" id="tm-spindrift" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/tm spindrift.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/rkXgws8fiDs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            While VideoQA Transformer models demonstrate competitive performance on standard benchmarks, the reasons behind their success are not fully understood. Do these models capture the rich multimodal structures and dynamics from video and text jointly? Or are they achieving high scores by exploiting biases and spurious features? 
          </p>
          <p>
            Hence, to provide insights, we design <b>QUAG</b> (QUadrant AveraGe), a lightweight and non-parametric probe, to conduct dataset-model combined representation analysis by impairing modality fusion. We find that the models achieve high performance on many datasets without leveraging multimodal representations. To validate QUAG further, we design <b>QUAG-attention</b>, a less-expressive replacement of self-attention with restricted token interactions. Models with QUAG-attention achieve similar performance with significantly fewer multiplication operations without any finetuning. <u>Our findings raise doubts about the current models' abilities to learn highly-coupled multimodal representations.</u> Hence, we design the <b>CLAVI</b> (Complements in LAnguage and VIdeo) dataset, a stress-test dataset curated by augmenting real-world videos to have high modality coupling. Consistent with the findings of QUAG, we find that most of the models achieve near-trivial performance on CLAVI. This reasserts the <u>limitations of current models for learning highly-coupled multimodal representations, that is not evaluated by the current datasets.</u>
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3"> <span class="coolname">QUAG</span>: Ablation of Modality Interactions</h2>

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Construct pixel-aligned features</h3> -->
        <div class="content has-text-justified">
          <p>
            QUAG ablates specific modality interactions (viz. unimodal, crossmodal, text, and video) by replacing the attention score values with the respective row-wise average values. Consequently, the performance drop is used to quantify the reliance of a model on the specific interactions on that dataset, thus facilitating joint dataset-model analysis. For instance, for a given dataset, if the model heavily relies only on unimodal information, ablating the unimodal component of modality fusion should significantly decrease performance. Also, this decrease in performance should be more pronounced than the decrease from ablating the crossmodal component of attention. <u>Therefore, performance drop is desirable. </u>
          </p>
          <img src="./static/images/quag_results.png" style= "max-width: 75%; display: block; margin: 0 auto;" />
          <p>
            We find that the JustAsk model does not learn to align and fuse the modalities. However, FrozenBiLM model consistently has strong reliance unimodal interactions and the text modality across the datasets but leverages cross-modal interactions only for ActivityNet-QA and MSRVTT-QA. <br>
            QUAG-attention averages the tokens <i>before</i> they are projected by the key matrix, hence reducing the number of multiplication operations. We provide more details in the main paper.
          </p>
        </div>
        <br/>
      </div>
    </div>

    <section class="section">
      <div class="container is-max-desktop">
    
        <div class="columns is-centered">
          <div class="column is-full-width">
            <h2 class="title is-3"> <span class="coolname">CLAVI</span>: Testing Through Complements</h2>
    
            <!-- Interpolating. -->
            <!-- <h3 class="title is-4">Construct pixel-aligned features</h3> -->
            <div class="content has-text-justified">
              <img src="./static/images/clavi_example_new.png" />
              <p>
                Inspired by <a href="https://bpiyush.github.io/testoftime-website/">Bagad et al.</a>, we developed CLAVI by modifying the sequence of events in video clips to enable isolated multimodal analysis. Unlike previous works, we incorporate both atemporal (existence-type) and temporal (before/after and beginning/end) question types, and propose consistent accuracy metrics for performance evaluation. Consequently, as illustrated in the figure, CLAVI comprehensively includes complements from both the language and video domains (question panels are colour-coded as per the correct answer; green:yes, red:no).
              </p>
              <img src="./static/images/clavi_results.png" style= "max-width: 80%; display: block; margin: 0 auto;" />
              <p>
                We find that while all the finetuned models are able to perform quite well on the <i>easy</i> multimodal instances, most of them have near-trivial performance on the <i>difficult</i> complement-subset.
              </p>
            </div>
            <br/>
          </div>
        </div>


<!-- <section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Integrating ConceptFusion with Large Language Models</h2>
    <div class="columns is-centered">
      <div class="column">
        <div class="content">
          <video id="gpt-video-1" autoplay controls muted playsinline height="100%">
            <source src="./static/videos/gpt video.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->


      <!-- <div class="column">
        <div class="content">
          <video id="gpt-video-2" autoplay controls muted playsinline height="100%">
            <source src="./static/videos/gpt video2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<!-- <section class="section" id="concurrent work">
  <div class="container is-max-desktop content">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Concurrent work</h2>

        <div class="content has-text-justified">
          <p>
            Given the pace of AI research these days, it is extremely challenging to keep up with all of the work around foundation models and open-set perception. We list below a few key approaches that we have come across after beginning work on ConceptFusion. If we may have inadvertently missed out on key concurrent work, please reach out to us over email (or better, open a pull request on <a href="https://github.com/concept-fusion/concept-fusion.github.io">our GitHub page</a>).
          </p>
          <p>
            <a href="https://mahis.life/clip-fields/">CLIP-Fields</a> encodes features from language and vision-language models into a compact, scene-specific neural network trained to predict feature embeddings from 3D point coordinates; to enable open-set visual understanding tasks.
          </p>
          <p>
            <a href="https://pengsongyou.github.io/openscene">OpenScene</a> demonstrates that features from pixel-aligned 2D vision-language models can be distilled to 3D, generalize to new scenes, and perform better than their 2D counterparts.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2211.16312">Deng et al.</a> demonstrate interesting ways of learning hierarchical scene abstractions by distilling features from 2D vision-language foundation models, and smart ways of interpreting captions from 2D captioning approaches.
          </p>
          <p>
            <a href="https://makezur.github.io/FeatureRealisticFusion/">Feature-realistic neural fusion</a> demonstrates the integration of DINO features into a real-time neural mapping and positioning system.
          </p>
          <p>
            <a href="https://semantic-abstraction.cs.columbia.edu">Semantic Abstraction</a> uses CLIP features to generate 3D features for reasoning over long-tailed categories, for scene completion and detecting occluded objects from language.
          <p>
            <a href="https://say-can.github.io/">Say-Can</a> demonstrates the applicability of large language models as task-planners, and leverage a set of low-level skills to execute these plans in the real world. Also related to this line of work are <a href="https://vlmaps.github.io/">VL-Maps</a>, <a href="https://nlmap-saycan.github.io/">NLMap-SayCan</a>, and <a href="https://cow.cs.columbia.edu/">CoWs</a>, which demonstrate the benefits of having a map queryable via language.
          </p>
          <p>
            <a href="lerf.io">Language embedded radiance fields (LERF)</a> trains a NeRF that additionally encodes CLIP and DINO features for language-based concept retrieval.
          </p>
          <p>
            <a href="https://vis-www.cs.umass.edu/3d-clr/">3D concept learning from multi-view images (3D-CLR)</a> introduces a dataset for 3D multi-view visual question answering, and proposes a concept learning framework that leverages pixel-aligned language embeddings from LSeg. They additionally train a set of neurosymbolic reasoning modules that loosely inspire our spatial query modules.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->


<!-- <script type="text/javascript">
  $(function() {
  var screenWidth = $(window).width();
  if (screenWidth >= 800) {
    $('#gpt-video-1').attr('autoplay', 'autoplay');
  }
  if (screenWidth >= 800) {
    $('#gpt-video-2').attr('autoplay', 'autoplay');
  }
  if (screenWidth >= 800) {
    $('#click-query-icl').attr('autoplay', 'autoplay');
  }
});
</script>  -->


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{rawal2024dissect,
author    = {Rawal, {Ishaan Singh} and Matyasko, Alexander and Jaiswal, Shantanu and Fernando, Basura and Tan, Cheston},
title     = {{Dissecting Multimodality in VideoQA Transformer Models by Impairing Modality Fusion}},
booktitle   = {International Conference on Machine Learning},
year      = {2024},
organization  = {PMLR}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./assets/pdf/2023-ConceptFusion.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/concept-fusion" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
      <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">
        <img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" />
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website adapted from the and <a href="https://concept-fusion.github.io">Concept Fusion</a> and Nerfies templates, which is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a href="https://github.com/dissect-videoqa/dissect-videoqa.github.io">source code</a> of this website,
            we just ask that you link back to the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies source code</a> in the footer.
            Please remember to remove the analytics code included in the header of the website which you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
